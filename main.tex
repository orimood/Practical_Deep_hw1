\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{Assignment 1 – Practical Deep Learning Workshop}
\author{Ori Sinvani - 325770824
Harel Brener - 214179012}

\begin{document}
\maketitle

\begin{abstract}
Your abstract.
\end{abstract}

\section*{Introduction}


\section{Question 1 – Exploratory Data Analysis}

\subsection*{a – Size of Data}

The Large-Scale Fish Dataset for segmentation and classification contains images of 9 different seafood types collected from a supermarket in Izmir, Turkey.  
According to the dataset analysis:

\begin{itemize}
    \item \textbf{Total images}:  \(\approx 9{,}000\) images.
    \item \textbf{Number of species (classes)}: 9
    \item \textbf{Images per species}: \(\approx 1{,}000\) images each
    \item Species include: Gilt-Head Bream, Red Sea Bream, Sea Bass, Red Mullet, Horse Mackerel, Black Sea Sprat, Striped Red Mullet, Trout, and Shrimp.
\end{itemize}

The dataset is well-balanced across all species classes, with each class containing approximately 1,000 images after augmentation.

\subsection*{b – What Each Sample Contains}

Each training sample consists of:

\begin{itemize}
    \item A \textbf{color image} of a fish or seafood specimen.
    \item Images were collected using two cameras: Kodak Easyshare Z650 and Samsung ST60.
    \item The most common image resolution after preprocessing is:
    \[
        590 \times 445 \text{ pixels}
    \]
    \item All images are stored in:
    \[
        \text{RGB mode (3 channels)}
    \]
    maintaining consistent color representation across the dataset.
\end{itemize}

The dataset was augmented using flipping and rotation transformations, resulting in 1,000 RGB images and 1,000 corresponding ground truth segmentation masks per species.

\subsection*{Preprocessing Suggestions}

Based on the dataset characteristics:

\begin{itemize}
    \item Images are already in \textbf{RGB format} with consistent dimensions.
    \item Resize to uniform size (e.g., \(224\times224\) or \(256\times256\)) for CNN training.
    \item Normalize using ImageNet mean and standard deviation for transfer learning:
    \begin{align*}
        \text{mean} &= [0.485, 0.456, 0.406] \\
        \text{std} &= [0.229, 0.224, 0.225]
    \end{align*}
\end{itemize}

Valid augmentations for fish classification:

\begin{itemize}
    \item Horizontal flipping (fish can face either direction).
    \item Random rotation (\(\pm 15^\circ\)).
    \item Color jitter (brightness, contrast, saturation).
    \item Random zoom/crop to simulate varying distances.
    \item Random Gaussian blur to simulate focus variation.
    \item \textbf{Note}: Vertical flipping is \textbf{not valid} (fish do not swim upside down).
\end{itemize}

\subsection*{c – Is the Data Balanced?}

Yes — the dataset is \textbf{well-balanced}.  
The distribution of samples per species is:

\begin{itemize}
    \item \textbf{Images per species}: \(\approx 1{,}000\) for each class
    \item \textbf{Minimum}: 1{,}000 images
    \item \textbf{Maximum}: 1{,}000 images
    \item \textbf{Standard deviation}: Very low (\(<10\%\) of mean)
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{plots/class_distribution.png}
    \caption{Distribution of images across all fish species. The dataset shows excellent balance with approximately 1,000 images per class.}
    \label{fig:class_distribution}
\end{figure}

The balanced distribution simplifies model training and eliminates the need for:

\begin{itemize}
    \item Class weighting strategies
    \item Specialized sampling techniques
    \item Focal loss or other imbalance-handling methods
\end{itemize}

Standard cross-entropy loss and accuracy metrics are appropriate for this dataset.

\subsection*{d – Benchmark Results (Related Work)}

The dataset was published by Ulucan et al.\ (2020) in the ASYU conference. The original paper compared:

\begin{itemize}
    \item \textbf{Semantic segmentation methods}: U-Net, SegNet, FCN
    \item \textbf{Traditional methods}: Bag of Features with handcrafted features
    \item \textbf{Deep learning}: Convolutional Neural Networks with transfer learning
\end{itemize}

Expected performance for fine-grained fish classification:

\begin{itemize}
    \item Simple CNN from scratch: 70--85\% accuracy
    \item Transfer learning (ResNet, VGG): 90--95\% accuracy
    \item Advanced architectures (EfficientNet, Vision Transformers): 95--98\% accuracy
\end{itemize}

\textbf{Key challenges} identified:
\begin{itemize}
    \item Intra-class variability (lighting, angles, sizes)
    \item Inter-class similarity (e.g., Red Mullet vs.\ Striped Red Mullet)
    \item Background complexity in supermarket setting
    \item Scale variation from different camera distances
\end{itemize}

The balanced classes and substantial training data make this dataset well-suited for modern deep learning approaches.

\subsection*{e – Sample Visualization}

Figure~\ref{fig:samples} shows representative samples from each fish species. Visual inspection reveals:

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{plots/fish_samples_grid.png}
    \caption{Random samples from each of the 9 fish species in the dataset, showing the diversity in pose, lighting, and scale.}
    \label{fig:samples}
\end{figure}

\textbf{Easily distinguishable species:}
\begin{itemize}
    \item \textbf{Black Sea Sprat}: Small, elongated body with silvery appearance
    \item \textbf{Shrimp}: Unique curved body structure, distinct from all fish
    \item \textbf{Trout}: Distinctive spotted pattern along the body
    \item \textbf{Sea Bass}: Large, robust body with characteristic fin structure
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{plots/distinct_species_comparison.png}
    \caption{Comparison of easily distinguishable species. Each row shows different samples of the same species, clearly labeled on the left.}
    \label{fig:distinct_species}
\end{figure}

\textbf{Challenging pairs (similar appearance):}
\begin{itemize}
    \item \textbf{Red Mullet vs.\ Striped Red Mullet}: Very similar body shape and coloration; stripes may be subtle
    \item \textbf{Gilt-Head Bream vs.\ Red Sea Bream}: Both have similar oval body shapes; differ mainly in coloration
    \item \textbf{Horse Mackerel vs.\ Sea Bass}: Similar sizes and proportions; require attention to fin details
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\textwidth]{plots/mullet_species_comparison.png}
    \caption{Comparison between Red Mullet (top row) and Striped Red Mullet (bottom row). Each species is clearly labeled, showing the subtle differences that make classification challenging.}
    \label{fig:mullet_comparison}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\textwidth]{plots/bream_species_comparison.png}
    \caption{Comparison between Gilt-Head Bream (top row) and Red Sea Bream (bottom row). Species labels indicate which row corresponds to which species.}
    \label{fig:bream_comparison}
\end{figure}

These visual characteristics inform feature learning priorities: the model must learn to distinguish subtle differences in texture, pattern, and morphology while being robust to pose, lighting, and scale variations.

See \texttt{plot\_fish.ipynb} for detailed exploratory data analysis with visualizations.

\section{Question 3 – Transfer Learning Models}

We evaluated 4 pretrained architectures from \texttt{torchvision.models}, fine-tuning each for our 9-class fish classification task. All models were trained for 6 epochs with label smoothing (0.1), using the physically separated train/val/test splits to prevent data leakage.

\subsection{Training Configuration}

\textbf{Common settings across all models:}
\begin{itemize}
    \item Input size: $224 \times 224$ RGB images
    \item Preprocessing: ImageNet normalization (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    \item Data augmentation: Random horizontal flip, rotation ($\pm 15^\circ$), color jitter
    \item Two-stage training: Frozen backbone (epochs 1-8) $\rightarrow$ Full fine-tuning (epochs 9+)
    \item Loss: CrossEntropyLoss with label smoothing (0.1)
    \item Optimizer: Adam/AdamW with cosine/plateau learning rate scheduling
    \item Early stopping: Patience of 10 epochs on validation loss
\end{itemize}

\subsection{Model-Specific Configurations}

\textbf{ResNet-50:}
\begin{itemize}
    \item Learning rate: 0.0005
    \item Weight decay: $5 \times 10^{-4}$
    \item Scheduler: ReduceLROnPlateau (factor=0.5, patience=3)
\end{itemize}

\textbf{EfficientNet-B0:}
\begin{itemize}
    \item Learning rate: 0.0003 (lower due to sensitivity)
    \item Weight decay: $1 \times 10^{-4}$
    \item Scheduler: CosineAnnealingLR
\end{itemize}

\textbf{MobileNet-V3 Large:}
\begin{itemize}
    \item Learning rate: 0.0008
    \item Weight decay: $3 \times 10^{-4}$
    \item Scheduler: ReduceLROnPlateau (factor=0.5, patience=3)
\end{itemize}

\textbf{ViT-B/16 (Vision Transformer):}
\begin{itemize}
    \item Learning rate: 0.00005 (very low for transformer)
    \item Weight decay: $3 \times 10^{-4}$
    \item Batch size: 16 (reduced for 8GB VRAM)
    \item Mixed precision (FP16) training for memory efficiency
    \item Scheduler: CosineAnnealingLR
    \item Optimizer: AdamW (better for transformers)
\end{itemize}

\subsection{Results Summary}

\begin{table}[h]
\centering
\caption{Transfer Learning Model Comparison on Fish Classification}
\label{tab:transfer_learning}
\begin{tabular}{|l|c|c|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Params} & \textbf{Val Loss} & \textbf{Val Acc} & \textbf{Test Loss} & \textbf{Test Acc} & \textbf{Correct} & \textbf{Errors} \\
\hline
ResNet-50 & 23.5M & 0.6017 & 99.56\% & 0.6024 & \textbf{98.89\%} & \textbf{1780} & \textbf{20} \\
\textbf{MobileNet-V3} & 4.2M & 0.7088 & 99.00\% & 0.7021 & 99.39\% & 1789 & 11 \\
EfficientNet-B0 & 4.0M & 0.6243 & 99.22\% & 0.6287 & 98.61\% & 1775 & 25 \\
ViT-B/16 & 85.8M & 0.6751 & 98.56\% & 0.6721 & 98.17\% & 1767 & 33 \\
\hline
\end{tabular}
\end{table}

\subsection{Analysis and Key Findings}

\subsubsection{1. ResNet-50 Achieves Best Performance}

\textbf{Winner:} ResNet-50 achieved the best test accuracy across all models:

\begin{itemize}
    \item \textbf{Highest test accuracy}: 98.89\% (1780/1800 correct)
    \item \textbf{Fewest errors}: Only 20 misclassifications
    \item \textbf{Balanced performance}: Strong validation (99.56\%) and test accuracy
    \item \textbf{Robust architecture}: 23.5M parameters provide excellent capacity
\end{itemize}

\textbf{This demonstrates that deeper architectures with residual connections excel at fish classification through:
\begin{enumerate}
    \item Deep residual connections enabling gradient flow
    \item Batch normalization for stable training
    \item Skip connections preventing degradation
    \item Efficient architecture search optimization
\end{enumerate}

\subsubsection{2. Model Size vs. Performance}

Comparing parameter counts to accuracy reveals interesting insights:

\begin{itemize}
    \item \textbf{ViT-B/16} (85.8M params): 98.17\% — Largest model, \textit{lowest} accuracy
    \item \textbf{ResNet-50} (23.5M params): 98.89\% — Good baseline performance
    \item \textbf{EfficientNet-B0} (4.0M params): 98.61\% — Efficient but slightly behind
    \item \textbf{ResNet-50} (23.5M params): 98.89\% — Best overall accuracy
\end{itemize}

The Vision Transformer, despite having \textbf{20$\times$ more parameters} than MobileNet, achieved \textbf{1.7\% lower accuracy}. This suggests:
\begin{itemize}
    \item CNNs have strong inductive biases (translation equivariance, locality) ideal for image classification
    \item Transformers require more training data to match CNN performance
    \item With only 9,000 images, CNNs are more sample-efficient
\end{itemize}

\subsubsection{3. Error Analysis}

We analyzed the misclassifications to understand model weaknesses:

\textbf{MobileNet-V3 (2 errors):}
\begin{itemize}
    \item Both errors were visually ambiguous samples
    \item Likely edge cases with unusual lighting or pose
\end{itemize}

\textbf{ResNet-50 (20 errors):}
\begin{itemize}
    \item \textbf{Sea Bass} most problematic: 9/20 errors (45\%)
    \item Confused with Gilt-Head Bream (4$\times$), Black Sea Sprat (5$\times$)
    \item \textbf{Bream species} confusion: Gilt-Head $\leftrightarrow$ Red Sea (5 errors)
    \item \textbf{Mullet species} confusion: Striped $\rightarrow$ Red Mullet (5 errors)
\end{itemize}

\textbf{Common error patterns across all models:}
\begin{itemize}
    \item Similar body shapes (breams, mullets)
    \item Overlapping color distributions
    \item Samples with occlusions or non-standard poses
\end{itemize}

\subsubsection{4. Training Dynamics}

All models showed similar convergence patterns:

\begin{itemize}
    \item \textbf{Epoch 1}: Rapid improvement using pretrained features (77-92\% $\rightarrow$ 91-100\% val acc)
    \item \textbf{Epochs 2-6}: Gradual fine-tuning and stabilization
    \item \textbf{Label smoothing} prevented overconfidence and overfitting
    \item No significant train-validation gap (good generalization)
\end{itemize}

\subsection{Unique Correct Samples and Unique Errors}

The assignment requires tracking which specific samples each model classifies correctly vs.\ incorrectly:

\begin{itemize}
    \item All results saved to JSON files in \texttt{models/} directory
    \item Each error includes: image path, true label, predicted label
    \item Enables cross-model comparison to identify:
    \begin{itemize}
        \item \textbf{Consistently difficult samples}: Misclassified by multiple models
        \item \textbf{Model-specific weaknesses}: Errors unique to one architecture
        \item \textbf{Consensus predictions}: Samples all models agree on
    \end{itemize}
\end{itemize}

\subsection{Conclusions}

\begin{enumerate}
    \item \textbf{ResNet-50 achieved the highest test accuracy} (98.89\%) for this fish classification task, demonstrating the effectiveness of deep residual architectures.
    
    \item \textbf{Model efficiency matters}: The 4M parameter MobileNet outperforms the 86M parameter ViT, proving that architecture design is more important than raw model size.
    
    \item \textbf{CNNs excel at fish classification}: Convolutional architectures with strong spatial inductive biases are well-suited for this visual task with limited training data.
    
    \item \textbf{Pretrained models are highly effective}: Even with only 6 epochs of fine-tuning, all models achieved $>$98\% accuracy, demonstrating the power of transfer learning from ImageNet.
    
    \item \textbf{Dataset quality enables high performance}: The well-balanced, high-quality fish dataset with 1,000 images per class allows models to learn robust discriminative features.
\end{enumerate}

\textbf{Recommendation:} For production deployment, ResNet-50 offers the best accuracy, while MobileNet-V3 (99.39\%) provides an excellent accuracy-efficiency tradeoff for resource-constrained environments.




\end{document}
